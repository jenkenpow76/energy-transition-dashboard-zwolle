{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6a350eb-f261-4b9c-809d-ae3c6f64d377",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Clean ISDE Subsidy Data\n",
    "This script cleans ISDE subsidy data downloaded from RVO.nl \n",
    "\n",
    "About the raw data:\n",
    "\n",
    "The ISDE subsidy data from RVO.nl was originally download as XLSX files. Upon initial inspection, it was noted that the files contained extra informational columns and rows that needed to be deleted. The information included images and plain text introduction material. The following original columns remain:\n",
    " - POSTCODE\n",
    " - PLAATS\n",
    " - GEMEENTENAAM\n",
    " - GEMEENTECODE\n",
    " - Realisatiejaar\n",
    " - WIJKNAAM\n",
    " - BUURTNAAM\n",
    " - DEELPROGRAMMA\n",
    " - SUBSIDIEJAAR\n",
    " - TECHNIEK\n",
    " - SUBCATEGORIE\n",
    " - AANTAL_ADRESSEN\n",
    " - TOTALE_VERMOGEN\n",
    " - AANTAL_M2\n",
    " - AANTAL_APP\n",
    " \n",
    "\n",
    "Processing and output:\n",
    "\n",
    "The file was saved as a CSV and used in the script below. The final output is generates a CSV file.\n",
    "\n",
    "## Before running:\n",
    "1. Update directories and file names as necessary under CONFIGURATION\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b715568b-831d-4870-8d76-06214b361289",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kemun\\AppData\\Local\\Temp\\ipykernel_11148\\2628474315.py:28: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  return df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèôÔ∏è Removed 811495 rows not from Zwolle.\n",
      "üîé Invalid postcode entries: 135\n",
      "üèòÔ∏è Subsidy counts per neighborhood:\n",
      " buurtnaam\n",
      "Berkum                           507\n",
      "Aa-landen-Noord                  323\n",
      "Wipstrik-Noord                   299\n",
      "Aa-landen-Oost                   286\n",
      "Gerenlanden                      281\n",
      "                                ... \n",
      "Bedrijventerrein Floresstraat      6\n",
      "Breecamp                           6\n",
      "Oude Mars                          4\n",
      "Noordereiland                      3\n",
      "Bedrijventerrein Voorst-D          2\n",
      "Name: count, Length: 67, dtype: int64\n",
      "‚úÖ Total rows after cleaning, filtering, and validation: 6326\n",
      "üìå Unique neighborhoods: 67\n",
      "üìÅ Cleaned and validated file saved to: ../clean_data/isde_subsidies_clean.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# -------------------------------\n",
    "# CONFIGURATION\n",
    "# -------------------------------\n",
    "RAW_DATA_DIR = \"../raw_data/\"\n",
    "OUTPUT_DIR = \"../clean_data/\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "input_file = os.path.join(RAW_DATA_DIR, \"ISDE_subsidies.csv\")\n",
    "output_file = os.path.join(OUTPUT_DIR, \"isde_subsidies_clean.csv\")\n",
    "\n",
    "# -------------------------------\n",
    "# FUNCTIONS\n",
    "# -------------------------------\n",
    "def clean_column_names(df):\n",
    "    df.columns = (\n",
    "        df.columns.str.strip()\n",
    "        .str.lower()\n",
    "        .str.replace(\" \", \"_\", regex=False)\n",
    "        .str.replace(\".\", \"_\", regex=False)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def strip_whitespace(df):\n",
    "    return df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "def clean_data(df):\n",
    "    df = strip_whitespace(df)\n",
    "    df = df.dropna(how='all')  # Drop entirely empty rows\n",
    "    df = df.drop_duplicates()\n",
    "    return df\n",
    "\n",
    "def remove_non_zwolle(df):\n",
    "    if 'plaats' in df.columns:\n",
    "        original_len = len(df)\n",
    "        df = df[df['plaats'].str.lower() == 'zwolle']\n",
    "        print(f\"üèôÔ∏è Removed {original_len - len(df)} rows not from Zwolle.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Column 'plaats' not found in the dataset.\")\n",
    "    return df\n",
    "\n",
    "def validate_by_neighborhood(df):\n",
    "    if 'buurtnaam' in df.columns:\n",
    "        buurt_counts = df['buurtnaam'].value_counts()\n",
    "        print(\"üèòÔ∏è Subsidy counts per neighborhood:\\n\", buurt_counts)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Column 'buurtnaam' not found. Skipping neighborhood validation.\")\n",
    "    return df\n",
    "\n",
    "def validate_postcodes(df):\n",
    "    if 'postcode_agg' in df.columns:\n",
    "        pattern = re.compile(r\"^\\d{4}\\s?[A-Z]{2}$\")  # Dutch postcode pattern\n",
    "        valid_postcodes = df['postcode_agg'].astype(str).str.upper().str.replace(\" \", \"\")\n",
    "        df['valid_postcode_agg'] = valid_postcodes.apply(lambda x: bool(pattern.match(x)))\n",
    "        \n",
    "        invalid_count = df['valid_postcode_agg'].value_counts().get(False, 0)\n",
    "        print(f\"üîé Invalid postcode entries: {invalid_count}\")\n",
    "        \n",
    "        df = df[df['valid_postcode_agg'] == True].drop(columns=['valid_postcode_agg'])\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Column 'postcode_agg' not found. Skipping postcode validation.\")\n",
    "    return df\n",
    "\n",
    "# -------------------------------\n",
    "# PROCESSING\n",
    "# -------------------------------\n",
    "df = pd.read_csv(input_file, sep=\",\", encoding=\"cp1252\")\n",
    "df = clean_column_names(df)  # <- Ensure lowercase headers early\n",
    "df = clean_data(df)\n",
    "df = remove_non_zwolle(df)\n",
    "df = validate_postcodes(df)\n",
    "df = validate_by_neighborhood(df)\n",
    "\n",
    "# -------------------------------\n",
    "# SUMMARY\n",
    "# -------------------------------\n",
    "print(f\"‚úÖ Total rows after cleaning, filtering, and validation: {len(df)}\")\n",
    "if 'buurtnaam' in df.columns:\n",
    "    print(f\"üìå Unique neighborhoods: {df['buurtnaam'].nunique()}\")\n",
    "\n",
    "# -------------------------------\n",
    "# SAVE OUTPUT\n",
    "# -------------------------------\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"üìÅ Cleaned and validated file saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225ba444",
   "metadata": {},
   "source": [
    "## Minimize ISDE Data\n",
    "The following code saves a minimized csv file with the following columns:\n",
    "   -postcode_agg\n",
    "    -wijknaam\n",
    "    -buurtnaam\n",
    "    -subsidiejaar\n",
    "    -techniek\n",
    "    -subcategorie\n",
    "\n",
    "## Before running:\n",
    "1. Update directories and file names as necessary under CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ac9da4d-2258-4592-90ff-84bf1aa9304d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Final dataset: 6326 rows\n",
      "üìÅ Output saved to: ../minimized_data/isde_subsidies_minimized.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# -------------------------------\n",
    "# CONFIGURATION\n",
    "# -------------------------------\n",
    "CLEAN_DATA_DIR = \"../clean_data/\"\n",
    "OUTPUT_DIR = \"../minimized_data/\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "input_file = os.path.join(CLEAN_DATA_DIR, \"isde_subsidies_clean.csv\")\n",
    "output_file = os.path.join(OUTPUT_DIR, \"isde_subsidies_minimized.csv\")\n",
    "\n",
    "# COLUMNS TO KEEP\n",
    "# -------------------------------\n",
    "COLUMNS_TO_KEEP = [\n",
    "    \"postcode_agg\",\n",
    "    \"wijknaam\",\n",
    "    \"buurtnaam\",\n",
    "    \"subsidiejaar\",\n",
    "    \"techniek\",\n",
    "    \"subcategorie\"\n",
    "]\n",
    "\n",
    "# -------------------------------\n",
    "# LOAD & FILTER\n",
    "# -------------------------------\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Ensure all required columns exist\n",
    "missing_cols = [col for col in COLUMNS_TO_KEEP if col not in df.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"‚ùå Missing columns in input file: {missing_cols}\")\n",
    "\n",
    "# Filter only the needed columns and rename postcode_agg to postcode\n",
    "df_minimized = df[COLUMNS_TO_KEEP].rename(columns={\"postcode_agg\": \"postcode\"})\n",
    "\n",
    "# -------------------------------\n",
    "# OUTPUT\n",
    "# -------------------------------\n",
    "print(f\"‚úÖ Final dataset: {len(df_minimized)} rows\")\n",
    "df_minimized.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"üìÅ Output saved to: {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb7ae81",
   "metadata": {},
   "source": [
    "## Before Running:\n",
    "1. Update directory and file names as necessary\n",
    "1. Uses Nominiatim open-source geocoding with OpenStreetMap data. View the license here: htttps://nominatim.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7860d4a-8aca-44ca-a956-b9b83c13ecdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geocoding complete. Saved as isde_subsidies_geocoded.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from geopy.geocoders import Nominatim\n",
    "import time \n",
    "from geopy.exc import GeocoderTimedOut\n",
    "\n",
    "#-----------------------------------------\n",
    "# CONFIGURATION\n",
    "#----------------------------------------\n",
    "\n",
    "CLEAN_DATA_DIR = \"../minimized_data/\"\n",
    "OUTPUT_DIR = \"../geocoded_data/\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "input_file = CLEAN_DATA_DIR + \"isde_subsidies_minimized.csv\"\n",
    "output_file = OUTPUT_DIR + \"isde_subsidies_geocoded.csv\"\n",
    "\n",
    "#----------------------------------------------\n",
    "# Load CSV file\n",
    "#----------------------------------------------\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "#----------------------------------------------\n",
    "# Ensure there's a 'postcode' column\n",
    "#----------------------------------------------\n",
    "if 'postcode' not in df.columns:\n",
    "    raise ValueError(\"CSV file must contain a 'postcode' column\")\n",
    "\n",
    "#-----------------------------------------\n",
    "# Use Nominatim open-source geocoder with OpenStreetMap data\n",
    "#------------------------------------------\n",
    "# Initialize the geocoder\n",
    "geolocator = Nominatim(user_agent=\"geo_script\")\n",
    "\n",
    "# Function to get latitude and longitude with retry logic\n",
    "def get_lat_lon(postcode, retries=3, delay=1):\n",
    "    try:\n",
    "        location = geolocator.geocode(postcode, country_codes=\"NL\")  # Replace \"XX\" with the actual country code\n",
    "        if location:\n",
    "            return location.latitude, location.longitude\n",
    "        else:\n",
    "            return None, None\n",
    "    except GeocoderTimedOut:\n",
    "        if retries > 0:\n",
    "            time.sleep(delay)\n",
    "            return get_lat_lon(postcode, retries - 1, delay * 2)  # Exponential backoff\n",
    "        else:\n",
    "            return None, None\n",
    "    finally:\n",
    "        time.sleep(1)  # Respect rate limits\n",
    "\n",
    "#----------------------------------------------    \n",
    "# Apply geocoding function\n",
    "#-----------------------------------------------\n",
    "# Apply geocoding function to postcodes\n",
    "df[['latitude', 'longitude']] = df['postcode'].astype(str).apply(lambda x: pd.Series(get_lat_lon(x)))\n",
    "\n",
    "#-----------------------------------------------\n",
    "# Save GeoCoded results to a new CSV file\n",
    "#-----------------------------------------------\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(\"Geocoding complete. Saved as isde_subsidies_geocoded.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8320ec2",
   "metadata": {},
   "source": [
    "# Aggregation based on neighbourhood\n",
    "\n",
    "This code aggregates subsidies occurence by performing a spatial join between geocoded subsidies data (sde_minimized_geocoded.csv) and neighborhood boundaries (Buurtgrenzen_Zwolle.shp). It calculates the total number of applied subsidies within each neighborhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b82c2442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Aggregated shapefile saved successfully to: ../aggregated_data/Zwolle_Neighbourhood_Aggregated_ISDESubsidy.shp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kemun\\AppData\\Local\\Temp\\ipykernel_11148\\786096978.py:41: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  result_gdf.to_file(output_path)\n",
      "c:\\Users\\kemun\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'subsidy_count' to 'subsidy_co'\n",
      "  ogr_write(\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "GEOCODED_DATA_DIR = \"../geocoded_data/\"\n",
    "NEIGHBORHOOD_DATA_DIR = \"../raw_data/\"\n",
    "OUTPUT_DIR = \"../aggregated_data/\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Define paths explicitly\n",
    "input_file = os.path.join(GEOCODED_DATA_DIR, \"isde_subsidies_geocoded.csv\")\n",
    "neighborhood_file = os.path.join(NEIGHBORHOOD_DATA_DIR, \"Buurtgrenzen_Zwolle.shp\")\n",
    "\n",
    "# Load ISDE subsidy data and convert to GeoDataFrame\n",
    "subsidy_df = pd.read_csv(input_file)\n",
    "subsidy_gdf = gpd.GeoDataFrame(\n",
    "    subsidy_df,\n",
    "    geometry=gpd.points_from_xy(subsidy_df.longitude, subsidy_df.latitude),\n",
    "    crs='EPSG:4326'\n",
    ")\n",
    "\n",
    "# Load neighborhoods shapefile with correct CRS (EPSG:28992)\n",
    "neighborhoods_gdf = gpd.read_file(neighborhood_file)\n",
    "\n",
    "# Convert subsidy points to match neighborhoods CRS (EPSG:28992)\n",
    "subsidy_gdf = subsidy_gdf.to_crs(neighborhoods_gdf.crs)\n",
    "\n",
    "# Spatial join: assign subsidies to neighborhoods\n",
    "joined_gdf = gpd.sjoin(subsidy_gdf, neighborhoods_gdf, predicate='within')\n",
    "\n",
    "# Count occurrences of subsidies per neighborhood\n",
    "aggregated_subsidies = joined_gdf.groupby('OFFICI√ãLE').size().reset_index(name='subsidy_count')\n",
    "\n",
    "# Merge aggregation results back with neighborhood geometry\n",
    "result_gdf = neighborhoods_gdf.merge(aggregated_subsidies, on='OFFICI√ãLE', how='left')\n",
    "result_gdf['subsidy_count'] = result_gdf['subsidy_count'].fillna(0).astype(int)\n",
    "\n",
    "# Save aggregated data to shapefile\n",
    "output_path = os.path.join(OUTPUT_DIR, \"Zwolle_Neighbourhood_Aggregated_ISDESubsidy.shp\")\n",
    "result_gdf.to_file(output_path)\n",
    "\n",
    "print(f\"‚úÖ Aggregated shapefile saved successfully to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dc24ae",
   "metadata": {},
   "source": [
    "# Aggregation based on district\n",
    "\n",
    "This code aggregates subsidies occurence by performing a spatial join between geocoded subsidies data (sce_minimized_geocoded.csv) and district boundaries (Wijkgrenzen_Zwolle.shp). It calculates the total number of applied subsidies within each district."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85dfe38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Aggregated shapefile saved successfully to: ../aggregated_data/Zwolle_District_Aggregated_ISDESubsidy.shp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kemun\\AppData\\Local\\Temp\\ipykernel_11148\\4218234446.py:41: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  result_gdf.to_file(output_path)\n",
      "c:\\Users\\kemun\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'subsidy_count' to 'subsidy_co'\n",
      "  ogr_write(\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "GEOCODED_DATA_DIR = \"../geocoded_data/\"\n",
    "DISTRICT_DATA_DIR = \"../raw_data/\"\n",
    "OUTPUT_DIR = \"../aggregated_data/\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Define paths explicitly\n",
    "input_file = os.path.join(GEOCODED_DATA_DIR, \"isde_subsidies_geocoded.csv\")\n",
    "district_file = os.path.join(DISTRICT_DATA_DIR, \"Wijkgrenzen_Zwolle.shp\")\n",
    "\n",
    "# Load ISDE subsidy data and convert to GeoDataFrame\n",
    "subsidy_df = pd.read_csv(input_file)\n",
    "subsidy_gdf = gpd.GeoDataFrame(\n",
    "    subsidy_df,\n",
    "    geometry=gpd.points_from_xy(subsidy_df.longitude, subsidy_df.latitude),\n",
    "    crs='EPSG:4326'\n",
    ")\n",
    "\n",
    "# Load districts shapefile with correct CRS (EPSG:28992)\n",
    "districts_gdf = gpd.read_file(district_file)\n",
    "\n",
    "# Convert subsidy points to match districts CRS (EPSG:28992)\n",
    "subsidy_gdf = subsidy_gdf.to_crs(districts_gdf.crs)\n",
    "\n",
    "# Spatial join: assign subsidies to districts\n",
    "joined_gdf = gpd.sjoin(subsidy_gdf, districts_gdf, predicate='within')\n",
    "\n",
    "# Count occurrences of subsidies per district\n",
    "aggregated_subsidies = joined_gdf.groupby('OFFICI√ãLE').size().reset_index(name='subsidy_count')\n",
    "\n",
    "# Merge aggregation results back with district geometry\n",
    "result_gdf = districts_gdf.merge(aggregated_subsidies, on='OFFICI√ãLE', how='left')\n",
    "result_gdf['subsidy_count'] = result_gdf['subsidy_count'].fillna(0).astype(int)\n",
    "\n",
    "# Save aggregated data to shapefile\n",
    "output_path = os.path.join(OUTPUT_DIR, \"Zwolle_District_Aggregated_ISDESubsidy.shp\")\n",
    "result_gdf.to_file(output_path)\n",
    "\n",
    "print(f\"‚úÖ Aggregated shapefile saved successfully to: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
